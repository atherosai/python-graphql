# -*- coding: utf-8 -*-
"""BERT fine-tunning in TensorFlow 2 with Keras API

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1934Mm2cwSSfT5bvi78-AExAl-hSfxCbq

# Installation

Google Colab offers free GPU and even TPU. For the purpose of simpler setup, we will stick to GPU. BERT models are quite big, so we need to be aware that we are constrainted by 12 GB of vRAM in Google Colab as Tesla K80 is used (as of 15/04/2020).

First let's check if you have GPU enabled in your session here in Colab. You can do it by running the following code.
"""

import tensorflow as tf

# Get the GPU device name.
device_name = tf.test.gpu_device_name()

# The device name should look like the following:
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')

"""If you do not have the GPU enabled, just go to:

`Edit -> Notebook Settings -> Hardware accelerator -> Set to GPU`

In order to fine-tune our model we need a couple of libraries to install first. 
TensorFlow 2 is already preinstalled, so the missing ones are [transformers](https://github.com/huggingface/transformers) and [TensorFlow datasets](https://github.com/tensorflow/datasets). This allow us to very easily import already pretrained models for TensorFlow 2 and fine-tune with Keras API.
"""

!pip install -q transformers tensorflow_datasets

"""# Loading IMDB dataset

We will use [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/). We are able to load it very quickly just with `tensorflow_datasets` library.

 return a tuple `(review, label)` instead of dictionary `{'example': example, 'label': label }`
"""

import tensorflow_datasets as tfds

(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', 
          split = (tfds.Split.TRAIN, tfds.Split.TEST),
          as_supervised=True,
          with_info=True)

print('info', ds_info)

"""Now let's explore the examples for fine-tunning. We can just take the top 5 examples and labels by `ds_train.take(5)`, so that we can explore the dataset without the need to iterate over whole 25000 examples in train dataset."""

for review, label in tfds.as_numpy(ds_train.take(5)):
    print('review', review.decode()[0:50], label)

"""# Tokenization

Now we need to apply BERT tokenizer to use pretrained tokenizers, see [HuggingFace Tokenizers](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer). The tokenizers should also match the core model that we would like to use as the pretrained, e.g. cased and uncased version. The code for loading the tokenizer is as simple as:
"""

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

"""The BERT tokenizer use WordPiece vocabulary. It has over 30000 words and it maps pretrained embeddings for each. Each word has its own ids, we would need to map the tokens to those ids."""

vocabulary = tokenizer.get_vocab()

print(list(vocabulary.keys())[5000:5020])

max_length_test = 20
test_sentence = 'Test tokenization sentence. Followed by another sentence'

# add special tokens

test_sentence_with_special_tokens = '[CLS]' + test_sentence + '[SEP]'

tokenized = tokenizer.tokenize(test_sentence_with_special_tokens)

print('tokenized', tokenized)

# convert tokens to ids in WordPiece
input_ids = tokenizer.convert_tokens_to_ids(tokenized)
  
# precalculation of pad length, so that we can reuse it later on
padding_length = max_length_test - len(input_ids)

# map tokens to WordPiece dictionary and add pad token for those text shorter than our max length
input_ids = input_ids + ([0] * padding_length)

# attention should focus just on sequence with non padded tokens
attention_mask = [1] * len(input_ids)

# do not focus attention on padded tokens
attention_mask = attention_mask + ([0] * padding_length)

# token types, needed for example for question answering, for our purpose we will just set 0 as we have just one sequence
token_type_ids = [0] * max_length_test

bert_input = {
    "token_ids": input_ids,
    "token_type_ids": token_type_ids,
    "attention_mask": attention_mask
}
print(bert_input)

"""These methods `tokenize` and `convert_token_to_ids` can be simplified into just one method called `encode_plus`, which will also add special tokens like `[CLS]` and `[SEP]` for us."""

bert_input = tokenizer.encode_plus(
                        test_sentence,                      
                        add_special_tokens = True, # add [CLS], [SEP]
                        max_length = max_length_test, # max length of the text that can go to BERT
                        pad_to_max_length = True, # add [PAD] tokens
                        return_attention_mask = True, # add attention mask to not focus on pad tokens
              )

print('encoded', bert_input)

"""This is taken from [HuggingFace glossary](https://huggingface.co/transformers/glossary.html):

**Input IDs** - The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.

**Attention mask** - Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]: 1 for tokens that are NOT MASKED, 0 for MASKED tokens.

**Token type ids** - Some modelsâ€™ purpose is to do sequence classification or question answering. These require two different sequences to be encoded in the same input IDs. They are usually separated by special tokens, such as the classifier and separator tokens. For example, the BERT model builds its two sequence input as such:

Before we can use such input for BERT fine-tunning, we need to be first aware that BERT is trained to consume sequence with maximum of 512 tokens. Let's define our **batch size** and **max length** allowed for our reviews.

# Encoding train and test dataset
"""

# can be up to 512 for BERT
max_length = 512

# the recommended batches size for BERT are 16,32 ... however on this dataset we are overfitting quite fast 
# and smaller batches work like a regularization. 
# You might play with adding another dropout layer instead.

batch_size = 6

"""Now let's combine the whole encoding process to one function so that we can map over our train and test dataset."""

def convert_example_to_feature(review):
  
  # combine step for tokenization, WordPiece vector mapping and will add also special tokens and truncate reviews longer than our max length
  
  return tokenizer.encode_plus(review, 
                add_special_tokens = True, # add [CLS], [SEP]
                max_length = max_length, # max length of the text that can go to BERT
                pad_to_max_length = True, # add [PAD] tokens
                return_attention_mask = True, # add attention mask to not focus on pad tokens
              )

"""When we will now iterate over again we can apply `encode` function for each item."""

# map to the expected input to TFBertForSequenceClassification, see here 
def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):
  return {
      "input_ids": input_ids,
      "token_type_ids": token_type_ids,
      "attention_mask": attention_masks,
  }, label

def encode_examples(ds, limit=-1):

  # prepare list, so that we can build up final TensorFlow dataset from slices.
  input_ids_list = []
  token_type_ids_list = []
  attention_mask_list = []
  label_list = []

  if (limit > 0):
      ds = ds.take(limit)
    
  for review, label in tfds.as_numpy(ds):

    bert_input = convert_example_to_feature(review.decode())
  
    input_ids_list.append(bert_input['input_ids'])
    token_type_ids_list.append(bert_input['token_type_ids'])
    attention_mask_list.append(bert_input['attention_mask'])
    label_list.append([label])

  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)

"""Now use our `encode_examples` function to convert our train and test datasets."""

# train dataset
ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)

# test dataset
ds_test_encoded = encode_examples(ds_test).batch(batch_size)

"""# Model initialization

We will use already prepared TensorFlow models from transformers models. You can just import them from the library and call `from_pretrained` and you will be able to use them
"""

from transformers import TFBertForSequenceClassification
import tensorflow as tf

# recommended learning rate for Adam 5e-5, 3e-5, 2e-5

learning_rate = 2e-5

# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 1


# model initialization
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')

# classifier Adam recommended
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)

# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

"""# Fine tunning

## Training

Now we can start fine-tunning process. We will again use Keras API `model.fit` and just pass the model configuration, that we have already defined.
"""

bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)

"""If you are getting the Resource error, you would need to run this on the GPU/TPU with higher VRAM. At least 12GB is recommended. You might consider to decrease input size/batch size or train in half precision.

## Evaluation

With our simple fine-tunning process, we have already achieved over 93% on test dataset. We likely underfit still. There are also other ways to improve further, e.g. larger BERT model, regularization, more epochs, further in task pretraining. 



```
4167/4167 [==============================] - 4542s 1s/step
- loss: 0.2456 - accuracy: 0.9024
- val_loss: 0.1892 - val_accuracy: 0.9326

```

This is quite good for the initial try in comparison with [current state of the art](https://nlpprogress.com/english/sentiment_analysis.html).

## Tips for fine-tunning

* Use larger BERT model
* Further in-task pretraining
* Multi-task fine-tunning has lower effect than further pretraining, but can help as well, see (Sun et al. 2019)
* Do not constrain yourself just to BERT other transformers like XLNet can work even better.

# References

* [Stanford CS224N: NLP with Deep Learning](https://https://www.youtube.com/watch?v=8rXD5-xhemo)

* [Stanford CS224U Natural Language Understanding](https://www.youtube.com/watch?v=tZ_Jrc_nRJY&list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)

* [HuggingFace transformers library](https://huggingface.co/)

* [BERT, ULMFit, ELMO](https://jalammar.github.io/illustrated-bert/)

* [BERTViz - Vizualization of Attention Heads](https://github.com/jessevig/bertviz)

* [Illustrated transformer](https://jalammar.github.io/illustrated-transformer/)

* [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2)

* http://nlpprogress.com/

* [GLUE benchmark ](https://gluebenchmark.com/tasks)

* Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,Bert:  Pre-training of deep bidirectional transformers for languageunderstanding, 2018.

* Scott Gray, Alec Radford, and Diederik P. Kingma,Gpu kernels forblock-sparse weights, 2017.

* Jeremy Howard and Sebastian Ruder,Universal language modelfine-tuning for text classification, 2018.

* Sepp Hochreiter and J Ìˆurgen Schmidhuber,Long short-term memory,Neural computation9(1997), no. 8, 1735â€“1780.

* Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, andIlya Sutskever,Language models are unsupervised multitask learners.

* Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang,How to fine-tunebert for text classification?, 2019.

* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, LlionJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin,Attentionis all you need, 2017.

* Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R. Bowman,Glue:  A multi-task benchmark and analysisplatform for natural language understanding, 2018.David Mraz (Atheros.ai)Transformers in TensorFlow 2April 15, 202013 / 15


* Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, RuslanSalakhutdinov, and Quoc V. Le,Xlnet:  Generalized autoregressivepretraining for language understanding, 2019
"""